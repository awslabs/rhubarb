{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Office Document Processing with Rhubarb\n",
    "\n",
    "This notebook demonstrates how to process Microsoft Office documents (Excel, PowerPoint, Word) using Rhubarb.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS credentials configured\n",
    "- Rhubarb installed with Office format dependencies: `pip install pyrhubarb`\n",
    "- Office documents to process\n",
    "\n",
    "The following dependencies are automatically installed with Rhubarb for Office format support:\n",
    "- `openpyxl` for Excel file processing\n",
    "- `python-pptx` for PowerPoint file processing  \n",
    "- `matplotlib` for visual rendering\n",
    "- `python-docx` for Word document processing (already supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from rhubarb import DocAnalysis\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ['AWS_PROFILE'] = 'aws-sample-works+prod-Admin'\n",
    "\n",
    "# Create a boto3 session\n",
    "session = boto3.Session()\n",
    "print(\"AWS session created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excel Spreadsheet Processing\n",
    "\n",
    "Rhubarb can process Excel files (.xlsx, .xls) with intelligent handling for large spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing an Excel file\n",
    "# Replace with path to your Excel file\n",
    "excel_file_path = \"/path/to/spreadsheet.xlsx\"\n",
    "\n",
    "# For demonstration, we'll show the setup without requiring an actual file\n",
    "try:\n",
    "    excel_analysis = DocAnalysis(\n",
    "        file_path=excel_file_path,\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    # Ask questions about the spreadsheet\n",
    "    response = excel_analysis.run(\n",
    "        message=\"What are the key data points in this spreadsheet? Summarize the main findings.\"\n",
    "    )\n",
    "    \n",
    "    print(\"Excel Analysis Result:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Excel file not found. Please update the file path to an actual Excel file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Specific Worksheets\n",
    "\n",
    "You can process specific worksheets by using the `pages` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process only specific worksheets (sheets 1, 2, and 3)\n",
    "try:\n",
    "    excel_specific = DocAnalysis(\n",
    "        file_path=excel_file_path,\n",
    "        pages=[1, 2, 3],  # Process first 3 worksheets\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = excel_specific.run(\n",
    "        message=\"Compare the data across these three worksheets. What patterns do you see?\"\n",
    "    )\n",
    "    \n",
    "    print(\"Specific Worksheets Analysis:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Excel file not found for specific worksheet processing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PowerPoint Presentation Processing\n",
    "\n",
    "Process PowerPoint presentations slide by slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing a PowerPoint file\n",
    "ppt_file_path = \"/path/to/powerpoint.pptx\"\n",
    "\n",
    "try:\n",
    "    ppt_analysis = DocAnalysis(\n",
    "        file_path=ppt_file_path,\n",
    "        pages=[1,3,5],\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    # Analyze the presentation\n",
    "    response = ppt_analysis.run(\n",
    "        message=\"Summarize the key points from this presentation. What are the main themes?\"\n",
    "    )\n",
    "    \n",
    "    print(\"PowerPoint Analysis Result:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"PowerPoint file not found. Please update the file path to an actual PPTX file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing PowerPoint file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Specific Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process specific slides\n",
    "try:\n",
    "    ppt_specific = DocAnalysis(\n",
    "        file_path=ppt_file_path,\n",
    "        pages=[1, 5, 10],  # Process slides 1, 5, and 10\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = ppt_specific.run(\n",
    "        message=\"What are the key messages on these specific slides?\"\n",
    "    )\n",
    "    \n",
    "    print(\"Specific Slides Analysis:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"PowerPoint file not found for specific slide processing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including PowerPoint Speaker Notes\n",
    "\n",
    "The new `include_powerpoint_notes` parameter allows you to optionally include speaker notes from PowerPoint presentations in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing PowerPoint with speaker notes included\n",
    "try:\n",
    "    ppt_with_notes = DocAnalysis(\n",
    "        file_path=ppt_file_path,\n",
    "        pages=[1, 2, 3],  # Process first 3 slides\n",
    "        include_powerpoint_notes=True,  # Include speaker notes\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = ppt_with_notes.run(\n",
    "        message=\"Analyze the content and speaker notes. What additional context do the notes provide?\"\n",
    "    )\n",
    "    \n",
    "    print(\"PowerPoint Analysis with Speaker Notes:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"PowerPoint file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Compare with notes disabled (default behavior)\n",
    "try:\n",
    "    ppt_without_notes = DocAnalysis(\n",
    "        file_path=ppt_file_path,\n",
    "        pages=[1, 2, 3],  # Same slides\n",
    "        include_powerpoint_notes=False,  # Default - no speaker notes\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = ppt_without_notes.run(\n",
    "        message=\"Analyze just the slide content without speaker notes.\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPowerPoint Analysis without Speaker Notes:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"PowerPoint file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data Extraction from Office Documents\n",
    "\n",
    "Use JSON schemas to extract structured data from Office documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for extracting structured data from a financial spreadsheet\n",
    "financial_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"total_revenue\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Total revenue amount\"\n",
    "        },\n",
    "        \"total_expenses\": {\n",
    "            \"type\": \"number\", \n",
    "            \"description\": \"Total expenses amount\"\n",
    "        },\n",
    "        \"net_profit\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Net profit (revenue - expenses)\"\n",
    "        },\n",
    "        \"key_metrics\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"metric_name\": {\"type\": \"string\"},\n",
    "                    \"value\": {\"type\": \"number\"},\n",
    "                    \"unit\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"total_revenue\", \"total_expenses\", \"net_profit\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Extract structured data from Excel file\n",
    "    structured_analysis = DocAnalysis(\n",
    "        file_path=excel_file_path,\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = structured_analysis.run(\n",
    "        message=\"Extract the financial data from this spreadsheet\",\n",
    "        output_schema=financial_schema\n",
    "    )\n",
    "    \n",
    "    print(\"Structured Data Extraction:\")\n",
    "    print(json.dumps(response[\"output\"], indent=2))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Excel file not found for structured extraction.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Document Processing\n",
    "\n",
    "For large Office documents, Rhubarb automatically handles chunking and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a large Excel file with sliding window\n",
    "large_file_path = \"/path/to/spreadsheet.xlsx\"\n",
    "\n",
    "try:\n",
    "    large_doc = DocAnalysis(\n",
    "        file_path=large_file_path,\n",
    "        sliding_window_overlap=2,  # Overlap between chunks\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    response = large_doc.run(\n",
    "        message=\"Analyze trends and patterns across this entire dataset\"\n",
    "    )\n",
    "    \n",
    "    print(\"Large Document Analysis:\")\n",
    "    print(response[\"output\"])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Large Excel file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Integration\n",
    "\n",
    "Process Office documents stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Office documents from S3\n",
    "s3_excel_path = \"s3://your-bucket/path/to/spreadsheet.xlsx\"\n",
    "s3_ppt_path = \"s3://your-bucket/path/to/presentation.pptx\"\n",
    "\n",
    "try:\n",
    "    # Excel from S3\n",
    "    s3_excel = DocAnalysis(\n",
    "        file_path=s3_excel_path,\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    excel_response = s3_excel.run(\n",
    "        message=\"What insights can you derive from this S3-stored spreadsheet?\"\n",
    "    )\n",
    "    \n",
    "    print(\"S3 Excel Analysis:\")\n",
    "    print(excel_response[\"output\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"S3 Excel processing error: {e}\")\n",
    "\n",
    "try:\n",
    "    # PowerPoint from S3\n",
    "    s3_ppt = DocAnalysis(\n",
    "        file_path=s3_ppt_path,\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    ppt_response = s3_ppt.run(\n",
    "        message=\"Summarize this S3-stored presentation\"\n",
    "    )\n",
    "    \n",
    "    print(\"S3 PowerPoint Analysis:\")\n",
    "    print(ppt_response[\"output\"])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"S3 PowerPoint processing error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Get real-time streaming responses for Office document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming analysis of Office documents\n",
    "try:\n",
    "    streaming_analysis = DocAnalysis(\n",
    "        file_path=excel_file_path,\n",
    "        boto3_session=session\n",
    "    )\n",
    "    \n",
    "    print(\"Streaming Excel Analysis:\")\n",
    "    for chunk in streaming_analysis.run_stream(\n",
    "        message=\"Provide a detailed analysis of the data trends in this spreadsheet\"\n",
    "    ):\n",
    "        # Note: For streaming, chunks don't have the same structure as run() responses\n",
    "        # Streaming chunks are usually strings or have different attributes\n",
    "        if hasattr(chunk, 'content'):\n",
    "            print(chunk.content, end='', flush=True)\n",
    "        elif isinstance(chunk, dict) and 'content' in chunk:\n",
    "            print(chunk['content'], end='', flush=True)\n",
    "        else:\n",
    "            print(chunk, end='', flush=True)\n",
    "    \n",
    "    print(\"\\n\\nStreaming complete.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"File not found for streaming analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\"Streaming error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Format Document Analysis\n",
    "\n",
    "Compare insights across different Office document formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple Office document types\n",
    "documents = {\n",
    "    \"Excel Report\": \"path/to/financial_report.xlsx\",\n",
    "    \"PowerPoint Summary\": \"path/to/executive_summary.pptx\", \n",
    "    \"Word Document\": \"path/to/detailed_analysis.docx\"\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for doc_type, file_path in documents.items():\n",
    "    try:\n",
    "        analysis = DocAnalysis(\n",
    "            file_path=file_path,\n",
    "            boto3_session=session\n",
    "        )\n",
    "        \n",
    "        response = analysis.run(\n",
    "            message=\"What are the key insights from this document?\"\n",
    "        )\n",
    "        \n",
    "        results[doc_type] = response[\"output\"]\n",
    "        print(f\"\\n{doc_type} Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response[\"output\"])\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"{doc_type} file not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {doc_type}: {e}\")\n",
    "\n",
    "print(\"\\nMulti-format analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Office Document Processing\n",
    "\n",
    "1. **File Size Management**: For very large Excel files, use page selection to process specific worksheets\n",
    "2. **Memory Efficiency**: Rhubarb automatically uses read-only mode for Excel files to optimize memory usage\n",
    "3. **S3 Integration**: Store large Office files in S3 for better performance and scalability\n",
    "4. **Error Handling**: Always implement proper error handling for file format and processing issues\n",
    "5. **Structured Extraction**: Use JSON schemas for consistent data extraction from Office documents\n",
    "\n",
    "## Supported Features\n",
    "\n",
    "- ✅ Excel (.xlsx, .xls) with automatic chunking for large files\n",
    "- ✅ PowerPoint (.pptx) with slide-by-slide processing\n",
    "- ✅ Word (.docx) with paragraph-based processing\n",
    "- ✅ S3 integration for all Office formats\n",
    "- ✅ Page/sheet/slide selection\n",
    "- ✅ Streaming responses\n",
    "- ✅ Structured data extraction\n",
    "- ✅ Large document processing with sliding window\n",
    "- ✅ Visual rendering at 150 DPI for optimal quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
